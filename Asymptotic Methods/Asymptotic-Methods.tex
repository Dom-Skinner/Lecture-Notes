\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{framed}
\usepackage{enumerate}
\usepackage{array}
\usepackage{amsfonts}
\usepackage[margin=3cm]{geometry}

\newcommand{\definition}{\textbf{Definition:}} 
\newcommand{\example}{\textbf{Example:}} 
\newcommand{\examples}{\textbf{Examples:}} 
\newcommand{\question}{\textbf{Question:}} 
\newcommand{\exercise}{\textbf{Exercise:}} 
\newcommand{\theorem}{\textbf{Theorem:}} 
\newcommand{\pder}[2] {\frac{\partial {#1}}{\partial {#2} }}%

\begin{document}

\title{Asymptotic Methods}
\author{Course given by Dr. D. Stuart \\
\LaTeX\  by Dominic Skinner \\
Dom-Skinner@github.com}
\maketitle
\tableofcontents
\noindent \textbf{Books:} Bender and Orszag, ``\emph{Advanced Mathematical methods for 
scientists and engineers}'', Chapters 3,6,10 \\
More details can be found on the Moodle course site; login to Moodle at
https://www.vle.cam.ac.uk/login/index.php, then self-enrol into the
Asymptotic methods course.
\\
\subsection*{What we'll learn in this course}
\textbf{Examples:} 
\begin{enumerate}[1.]
\item $\displaystyle I(\lambda) = \int_{\infty}^{\infty} \exp[-\lambda \cosh u] du $
      \\ We expect that $I(\lambda) \to 0$ as $\lambda \to \infty$. But how fast?
\\
\item $\displaystyle i \hbar \pder{\psi}{t} = -\frac{\hbar^2}{2m}
      \pder{^2 \psi}{x^2} + V(x)$ with $\psi(x,t) \in \mathbb{C}$,
      $V = V(x)$. \\
      Look for a solution $\displaystyle \psi(x,t) = \exp \left[\frac{-iEt}{\hbar} 
      \right] f(x)$
      $\implies \hbar^2 f'' = 2m(V(x) -E)f$ 
      \\
      $\hbar$ is very small. So a natural problem is to try and understand
      $\displaystyle \epsilon^2 \frac{d^2y}{dx^2} = Q(x)y$ when $\epsilon \ll 1$.
      \\ The ``semi-classical limit'' or ``geometric optics''.
\\
\item Put $\hbar = 1$, $m=\frac{1}{2}$, $V=0$; specify $\psi(x,0) = \psi_0(x)$
      \\[2pt]
      Fourier transform $\displaystyle \to \psi(x,t) = \frac{1}{(4\pi i t)^{1/2}} 
      \int_{\mathbb{R}} \exp\left[ \frac{i|x-y|^2}{4t} \right] \psi_0(y) dy$.
      \\
      \\
      Question: Does $\psi(x,t)$ really approach $\psi_0(x)$ as $t\to0$?
\end{enumerate}
%
\section{Asymptotic expansions of functions}
\[ \sinh x = \frac{e^x - e^{-x}}{2} = x + \frac{x^3}{3!} + \frac{x^5}{5!} + \dots\]
say $\sinh x \sim x$ as $x \to 0$.
\\
\\
\definition\ $f \sim g$ as $x \to x_0$ is $|f(x) - g(x)| = o(g(x))$ as $x \to x_0$.
\\
\\
\example 
\[|\sinh x - x| = |\frac{x^3}{3!} + \frac{x^5}{5!} + \dots| = O(x^3) = o(x)\]
\\
($F=O(G)$ as $x \to x_0$ means $\exists \, C>0$ such that $|F(x)| \leq C|G(x)|$ in
some open interval $I$, with $x_0 \in I$)
\\
In fact, by remainder estimate for Taylor expansion
\[ \left| \sinh x - \sum_{0}^{N} \frac{x^{2n+1}}{(2n+1)!} \right| = \
O(x^{2n+3}) = o(x^{2n+1}) \mbox{ as } x \to 0\]
We write $\displaystyle \sinh x \sim \sum_{0}^{\infty} \frac{x^{2n+1}}{(2n+1)!}$
\\
\\
\definition\ Asymptotic sequence and asymptotic expansion.
\begin{enumerate}[(i)]
\item $\{\phi_n\}_{n=0}^{\infty}$ is an asymptotic sequence (of functions) as 
		$x \to x_0$ if $\phi_{n+1}(x) = o(\phi_n(x))$ as $x \to x_0$. 
\item A function f has asymptotic expansion w.r.t. $\{ \phi_n\}$ as $x \to x_0$ 
		written $f \sim \sum_{n=0}^{\infty} a_n \phi_n$ if 
		\[ \left| f(x) - \sum_{n=0}^N a_n \phi_n(x) \right| = o(\phi_N(x)) \mbox{ as }
		 x \to x_0 \forall N\]
		\\
		Notice the difference with Taylor expansion - an asymptotic expansion 
		need not converge as $N\to \infty$ for any $x$!
\end{enumerate}
\examples\ 
\begin{itemize}
\item $\{ \phi_n(x) = x^n \}$ as $x \to 0$, the most common sequence.
\item $\{ \phi_n(x) = x^{2n+1} \}$ as $x \to 0$
\item $\{ \phi_n(x) = e^{-n/x} \}$ as $x \to 0^+$ (i.e. $x>0$ and $x\to 0$ on right)
\end{itemize}
%
%
\begin{framed}
\noindent \begin{tabular}{cl} 
\textbf{Warning:} & $\displaystyle \sin x \sim x - \frac{x^3}{3!} -
\frac{x^5}{5!} + \dots$ as $x \to 0$. \\
& $\displaystyle \sin x + e^{-1/x} \sim x  - \frac{x^3}{3!} + \frac{x^5}{5!}
 - \dots$ as $x \to 0^{+}$. 
\end{tabular}
\end{framed}
Why?
\[ \left| \sin x + e^{-1/x} - \sum_0^N \frac{ (-1)^n x^{2n+1}}{(2n+1)!} \right| \
= \left| \sum_{n = 2N+1}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!} + e^{-1/x} \right| \
=O(x^{2N+3}) = o(x^{2N+1}) \]
Moral: information is lost in asymptotic expansions!
\\
\\
However, given $f$ and asymptotic sequence, the $a_j$'s are unique, i.e.
\begin{align*}
a_0 &= \lim_{x \to x_0} \frac{f(x)}{\phi_0(x)} \\
a_1 &= \lim_{x \to x_0} \frac{f(x)-a_0 \phi_0(x)}{\phi_1(x)} \\
& \vdots
\end{align*}
\question\ Is it possible that $f(x) \sim 0$ as $x \to 0$?
\\
If $|f(x) - 0| = o(0)=0$ in some interval $I$, containing $0$, then
$f \equiv 0$ on $I$.
\\
\\
\example\ Consider $\displaystyle E_1(x) = \int_{x}^{\infty} \frac{e^{-t}}{t} dt$
as $x \to + \infty$.
\\
\\
Consider the asymptotic sequence $\phi_n(x) = 1/x^n$ as $x \to + \infty$
\[ E_1(x) = \int_{x}^{\infty} \frac{-d(e^{-t})}{t} = \left[ -\frac{e^{-t}}{t} \right]_{x}^{\infty} \
- \int_x^{\infty} \frac{e^{-t}}{t^2} dt = \frac{e^{-x}}{x} - \
\int_x^{\infty} \frac{e^{-t}}{t^2} dt\]
Claim: $E_1(x) \sim e^{-x}/x$ as $x \to + \infty$.
\[ \left| E_1(x) - \frac{e^{-x}}{x} \right| = \left| \int_x^{\infty} \frac{e^{-t}}{t^2} dt \right|
\leq \frac{1}{x^2} \int_x^{\infty} e^{-t} dt = \frac{e^{-t}}{x^2} = \
o\left(\frac{e^{-x}}{x} \right) \]
Working out the full expansion of $E_1$ with respect to
$\phi_n = e^{-x}/x^n$ gives that.
\[ E_1(x) \sim \sum_{n=0}^{\infty} \frac{(-1)^n n! e^{-x}}{x^{n+1}} \]
\underline{What do we mean?}
\begin{enumerate}[(i)]
\item $\phi_n (x) - e^{-x}/x^{n+1}$ satisfies
		$\phi_{n+1}(x) = o(\phi_n(x))$ as $x \to + \infty$.
		i.e. it forms an ``asymptotic sequence.''
\item The notation ``$\sim$'' (``asymptotic to'') means
\[ \left|E_1(x) - \sum_{n=0}^N \frac{(-1)^n n! e^{-x} }{x^{n+1}} \right| = o\left(
\frac{e^{-x}}{x^{n+1}} \right) \mbox{ as } x \to + \infty\]
\end{enumerate}
This can be proved with integration by parts:
\begin{align*}
E_1(x) &= -\int_x^{\infty} \frac{1}{t} d(e^{-t}) = e^{-x}/x + \int_x^{\infty}
\frac{1}{t^2} d(e^{-t}) \\
&= e^{-x}/x - e^{-x}/x^2 + 2 \int_x^{\infty} \frac{e^{-t}}{t^3}dt  \\
&= e^{-x} \left[ \frac{1}{x} - \frac{1}{x^2} + \frac{2!}{x^3} - \frac{3!}{x^4}
+ \dots + \frac{(-1)^n n!}{x^{n+1}} \right] + \underbrace{(-1)^{n+1} (n+1)!
\int_x^{\infty} \frac{e^{-t}}{t^{n+2}} dt}_{Rem_{n+1}(x)}
\end{align*}
Where
\[ |Rem_{n+1}(x)| \leq \frac{(n+1)!}{x^{n+2}} \int_x^{\infty} e^{-t} dt =
\frac{(n+1)! e^{-x}}{x^{n+2}} = o\left( \frac{e^{-x}}{x^{n+1}} \right) 
\mbox{ as } x \to + \infty \]
So it is an asymptotic expansion. Not convergent because 
$\displaystyle \sum (-1)^n \,n! \, y^{n+1}$
has radius of convergence 0. (In fact for fixed $y$ the terms become unbounded.)
\\
\\
Consider magnitudes of successive terms $\displaystyle 
f_n(x) =\frac{ (-1)^n\, n!\, e^{-x}}{x^{n+1}}$
\[ \left| \frac{f_{n+1}(x)}{f_n(x)}\right| = \frac{n+1}{x} \]
% A figure goes here
\underline{Optimal truncation:} Truncate the asymptotic expansion at the 
point $n=N_x$, such that the first term excluded is the smallest.
\\
\\
In our example, choose $N_x = [x] -1 = \sup \{j-1: j \leq x, \, j\in \mathbb{N} \}$
\[ \left. \begin{array}{ccc}
\left| f_{N_x+1}(x)/f_{N_x}(x)\right| &= (N_x+1)/x &\leq 1 \\
\\
\left| f_{N_x+2}(x)/f_{N_x+1}(x)\right| &= (N_x+2)/x &> 1 \end{array}
\right\} \mbox{ so } f_{N_x+1} \mbox{ is the smallest term, later terms are larger} \]
So we write
\[ E_1(x) = \sum^{N_x} \frac{(-1)^n n! e^{-x}}{x^{n+1}} + Rem_{N_x+1}(x) \]
\[ |Rem_{n+1}(x)| \leq \frac{(N_x+1)!}{x^{N_x+2}}e^{-x} =
\frac{[x]! e^{-x}}{x^{[x]+1}} \leq
\frac{2 \left( \frac{[x]}{e} \right)^{[x]} \sqrt{2\pi [x]} e^{-x}}{x^{[x]+1}}
\leq \frac{2\sqrt{2\pi[x]}}{[x]} e^{-x} e^{-[x]} \]
Where we have used Stirling's formula. 
\[ \lim_{n \to \infty} \frac{n!}{(n/e)^n \sqrt{2\pi n}} \to 1 \mbox{ as } n \to \infty \]
The good new is the additional 
$e^{-[x]}$ term. Optimal truncation (often) gives an exponentially small 
remainder.
\\
\\
\examples\
\[ \sinh x = \frac{e^x - e^{-x}}{2} \sim e^{-x}/2 \mbox{ as } x \to + \infty \]
Works because $e^{-x} = o(e^x)$ as $x \to \infty$
\\
\\
\[ \mathrm{sech}  \, x = \frac{2}{e^x + e^{-x}} = \frac{2}{e^x}(1 + e^{-2x})^{-1} =
\frac{2}{e^x}(1 - e^{-2x} + e^{-4x} - \dots) \]
This gives an asymptotic expansion for the sequence $\phi_n = e^{-nx}$
(Which is asymptotic since $e^{-x} = o(e^x)$ as $x \to + \infty$)
\\
\\
Note: $\sinh x \sim -e^{-x}/2$ as $x \to -\infty$
\\
\\
Consider $\sinh z, \mbox{ for }\, z \in \mathbb{C} $
\[ \sinh z = \frac{e^z- e^{-z}}{2} = \frac{e^{x+iy} - e^{-x-iy}}{2} \]
\begin{alignat*}{2}
& \sim  e^z/2 &\mbox{ as } z \to \infty \mbox{ in sector } \{ -\frac{\pi}{2}
< arg(z) < \frac{\pi}{2} \} \\
& \sim  e^{-z}/2 &\mbox{ as } z \to \infty \mbox{ in sector } \{ \frac{\pi}{2}
< arg(z) < \frac{3\pi}{2} \} 
\end{alignat*}
\underline{Conclusion:} The asymptotic seems to change suddenly when going from
sector to sector. % Figure of said sectors
\\
\\
The lines separating the different sectors are Stokes Lines.
\\
\\
\textbf{Excercise:} Prove that the definition of asymptotics in a sector must satisfy
that you do not approach Stokes lines too fast.
\\
\\
\textbf{Terminology:} 
\[ \left. \begin{array}{rc}
e^{z}/2 & \mbox{dominant} \\
-e^{z}/2 & \mbox{subdominant or recessive} \end{array} \right\} \mbox{ for Arg }
z \in ( - \frac{\pi}{2} , \, \frac{\pi}{2} )\]
On Stokes lines, neither of these terms is dominant. This means that the asymptotic 
relation holds if ``\emph{$z \to \infty$ but not approaching Stokes lines.}''
\\
\\
Why? Consider $z_n = 1/n + in^2$ has Re $z_n >0$, and $|z_n| \to \infty$.
\[ \sinh z_n = \frac{1}{2} \left( e^{1/n + in^2} - e^{-1/n -in^2} \right)\]
where
\[ \sinh z_n \sim \frac{1}{2} e^{1/n + in^2} \]
means that 
\[ \left| \sinh z_n - \frac{1}{2} e^{1/n + in^2} \right| = \frac{1}{2} \left| e^{-1/n -in^2}
\right| = \frac{1}{2} e^{-1/n}\]
but $e^{-1/n} \neq o \left( e^{1/n} \right)$ as $n\to \infty$.
So we must consider $z \to \infty$ with Arg $z \in [ -\frac{\pi}{2} + \epsilon, 
\frac{\pi}{2} - \epsilon ] $for some $\epsilon >0$
\\
\\
\definition\ 
\begin{equation}\tag{$z\to \infty, \, Arg \, z \in (\alpha, \beta) $}
f(z) \sim a_0 + \frac{a_1}{z} + \frac{a_2}{z^2} + \dots
\end{equation}
means given $N \in \mathbb{N}$, $\epsilon >0$ sufficiently small
\[\left| f(z) - \sum_{i=0}^{N} \frac{a_i}{z^i} \right| = o(z^{-N})\]
as $z \to \infty$ Arg $z \in [ \alpha + \epsilon , \beta - \epsilon ]$
In this case we write
\[ f(z) = \sum_{i=0}^{N} \frac{a_i}{z^i} \qquad (z \to \infty; \, Arg \, z \in
(\alpha,\beta) )\]
\exercise\ Write out corresponding definition for 
\[ f(z) = \sum_{i=0}^{N} a_i(z-z_0)^i \qquad (z \to z_0; \, Arg \,( z- z_0) \in
(\alpha,\beta) )\]
\example\ 
\[ \sinh \frac{1}{z} \sim \frac{1}{2} e^{1/z} \qquad (z \to 0; \, Arg \, z \in 
(-\frac{\pi}{2}, \frac{\pi}{2} ) ) \]
but 
\[ \sinh \frac{1}{z} \sim -\frac{1}{2} e^{-1/z} \qquad (z \to 0; \, Arg \, z \in 
(\frac{\pi}{2}, \frac{3\pi}{2} ) ) \]
(fails on Stokes line)
\\
\\
Consider complex analytic (holomorphic) functions:
\[ f(z) = \sum _{n=0}^{\infty} c_n (z - z_0)^n\]
if f is holomorphic near $z_0$ and then 
\[ f(z) \sim \sum _{n=0}^{\infty} c_n (z - z_0)^n \qquad (z \to z_0; \, Arg \, z 
\in [0, 2\pi])\]
Fact: If $z_0$ is an isolated singularity, i.e. $f$ is holomorphic in 
$\{ z : 0 < |z-z_0| < r \}$ for some $r>0$ and 
\[ f(z) \sim \sum _{n=0}^{\infty} c_n (z - z_0)^n \qquad (z \to z_0; \, Arg \, z 
\in [0, 2\pi])\]
then $z_0$ is a removable singularity, and $\sum a_n (z-z_0)^n$ converges to
$f$ in some neighbourhood of $z_0$.
\\
\\
Why? At isolated singularity $z_0$ either 
\begin{itemize}
\item removable $\implies |f(z)|$ bounded as $z\to z_0$ 
\item Pole $f(z) = \frac{a_{-N}}{(z-z_0)^N} + \frac{a_{-N+1}}{(z-z_0)^{N-1}} + \dots$
\item essential singularity $f(z) = \sum_{-\infty}^{\infty} a_j(z-z_0)^j$
      where some $a_{-n} \neq 0$ for arbitrarly large n
\end{itemize}
The last two have $|f(z)|\to \infty$ as $z \to z_0$ on some sequence.
\\
\\
Notice that if $f(z) \sim \sum_0^{\infty} a_n (z-z_0)^n$ holds for all Arg $z$ then
$|f(z) - a_0| = o(1)$ as $z \to z_0$ $\forall$ Arg$(z-z_0)$. I.e. $|f(z)|$
is bounded as $|z-z_0| \to 0$.
\\
\\
Therefore $z_0$ is a removable singularity, i.e. $f$ is analytic at $z_0$. So
$f(z) = \sum a_n (z-z_0)^n$ by uniqueness of asymptotic expansions. So we will
often end up considering asymptotic expansions at essential singularities. 
\\
\subsection*{Differential Equations}
$\sinh x$ solves $\displaystyle \frac{d^2y}{dx^2} = y$, with $y(0)=0$, 
$y'(0)=1$. $x=0$ is an ``ordinary point''.
\\
\\
Recall:
\[ y'' + C_1(x) y' + C_0(x) y =0 \]
$x=0$ is an ordinary point if
\[ C_j(x) = \sum_0^{\infty} c_{j_n}x^n, \quad j = 0,1\]
(and these are convergent)
\\
$x=0$ is a regular singular point if 
\[ C_1(x) = \frac{1}{x} P_1(x), \quad C_0(x) = \frac{1}{x^2}P_0(x) \]
where $\displaystyle P_j = \sum_0^{\infty} P_{j_n} x^n, \quad j=0,1$
(and these are convergent).
\\
(This is also written $x^2 y'' + xP_1(x) y' + P_0(x)y =0$)
\\
\\
Let's see heuristically why $y$ can be singular at a regular point but
not an ordinary point. Assume
\[ y = b_0 x^a + b_1 x^{a+1} + \dots \]
with $a<0$.
\begin{align*}
 y' &= ab_0 x^{a-1} + (a+1)b_1 x^{a} + \dots \\
 y'' &= a(a-1)b_0 x^{a-2} + (a+1)ab_1 x^{a-1} + \dots 
\end{align*}
There is no possibility to balance the ``worst terms'' $x^{a-2}$ at
an ordinary point. At a regular singular point we can hope to balance the 
worst terms because
\[ x^2 x^{a-2} = x^a \]
Consider the Airy equation
\begin{equation}\tag{A}
\frac{d^2 y}{dx^2} = xy
\end{equation}
$x=0$ is an ordinary point. The power series solution at an ordinary point is
\[ y(x) = \sum_{n=0}^{\infty} a_n x^n \]
This can be solved to get the relations
\begin{alignat*}{3}
n(n-1)a_n &=0 \quad  &n &= 0,1,2 \\
n(n-1)a_n &= a_{n-3} \quad &n &= 3,4,\dots
\end{alignat*}
Therefore we have two linearly independent solutions
\[ y_0 = a_0 \sum_{n=0}^{\infty}\frac{x^{3n}}{9^n n! \Gamma(n + \frac{2}{3})} ; \quad
y_1 = a_1 \sum_{n=0}^{\infty}\frac{x^{3n+1}}{9^n n! \Gamma(n + \frac{4}{3})} \]
(Can be easily seen that $\displaystyle a_n = \frac{a_{n-3}}{n(n-1)} \implies a_{3n} =
\frac{a_{3n-3}}{3n(3n-1)}= \dots = \frac{a_0}{3^n n! 3^n (n-\frac{1}{3})
(n-\frac{4}{3}) \cdots \frac{2}{3}} $ )
The radius of convergence is $\infty$. (In general the radius of convergence of a
power series solution at an ordinary point is the distance to the closest 
singularity.)
\\
\\
\question\ Is $x= + \infty$ a singular point for (A)?
\\
Also recall solutions of $y'' = y$, $y_0 = \cosh x, \; y_1 = \sinh x$. to analyse the
point at infinity, let $z = 1/x$.
\begin{align*}
y(x) = w(1/x) &= w(z) \\
\implies y'(x) &= - \frac{1}{x^2} w'(1/x) \\
y''(x) &= \frac{1}{x^4} w''(1/x) + \frac{2}{x^3} w' ( 1/x)
\end{align*}
\[\left.  \begin{array}{ll}
y''(x) = y(x) &\iff z^4 w'' + 2 z^3 w' = w \\
y''(x) = xy(x) &\iff z^4 w'' + 2 z^3 w' = \frac{1}{z}w \end{array}
\right\} \mbox{ So an irregular singular point} \] 
Often get essential singularities.
\\
\subsubsection*{Behaviour of solutions as $x \to \infty$}
Think of (A) as like $y'' = ay$ (which has solutions $\exp(\pm a^{1/2} x)$)
but with $a$ increasing with x. So \underline{maybe} the solution looks like
a speeded up exponential, $\exp[S(x)]$, where 
\begin{equation}\tag{G}
S(x) \sim c x^{\frac{3}{2}} + \dots
\end{equation}
\[ y'' = xy \iff S'' + (S')^2 =x \]
If (G) holds, and also
\begin{align*}
S' & \sim \frac{3c}{2} x^{\frac{1}{2}} + \dots \\
S'' & \sim \frac{3c}{4} x^{-\frac{1}{2}} + \dots 
\end{align*}
Dominant balance:
\[ (S_0')^2 = x \implies S'_0 = \pm x^{1/2} \implies S_0 = \pm \frac{2}{3} x^{3/2} +c \]
Now substitute $S_0$ into the discarded $S''$ term and generate a new improved solution
$S_{NI}$ by solving $(S_{NI}')^2 = x - S_0'' = x\mp \frac{1}{2} x^{-1/2} $ 
\begin{align*}
S_{NI}' &= \pm x^{1/2} \left( 1 \mp \frac{1}{2} x^{-3/2} \right)^{1/2}  \\
 &= \pm x^{1/2} - \frac{1}{4} x^{-1} + O(x^{-5/2}) 
 \end{align*}
Therefore
\[S_{NI}(x) = \pm \frac{2}{3}x^{3/2} - \frac{1}{4} \log x + O(x^{-3/2}) \]
This suggest solutions of (A) look like
\begin{align*}
y_+(x) & \sim x^{-1/4} \exp \left[ + \frac{2}{3} x^{2/3} \right] \\
y_-(x) & \sim x^{-1/4} \exp \left[ - \frac{2}{3} x^{2/3} \right] 
\end{align*}
Note:
\begin{itemize}
\item Make informed guess
\item throw away what you hope is small and solve the resulting equation
      ($(S_0')^2 =x$)
\item Check for self consistency ($S_{NI} = \pm \frac{2}{3} x^{3/2} - 
      \frac{1}{4} \log x  +\dots$)
\end{itemize}
\subsubsection*{Bessel equation}
\[ x^2 y'' + xy' - (x^2+\nu^2)y =0 \]
\emph{Revise power series for this, look at operations on asyptotic
expansions \S\ 3.8 Bender-Orsag and Q1 on Ex Sheet 1 }
\\
We can add, multiply, take reciprocals and integrate asymtotic expansions.
But \underline{Cannot differentiate in general}. For example
\[ f(x) \sim x^{-1/2} + x^{-3/2} \sin x^{x^3} \]
\end{document}
